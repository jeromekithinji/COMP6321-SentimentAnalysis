{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiyj8akVFNRVxXzjyn4UuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeromekithinji/COMP6321-SentimentAnalysis/blob/main/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfR4qswFjnCR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP & GLOVE DOWNLOAD\n",
        "if not os.path.exists('glove.6B.100d.txt'):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"GloVe embeddings already available.\")\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
      ],
      "metadata": {
        "id": "CT28I5qzkAA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "EMBEDDING_DIM = 100\n",
        "CSV_FILE_PATH = 'cleaned_data.csv'\n"
      ],
      "metadata": {
        "id": "6huZbI1xkE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    df = pd.read_csv('/content/cleaned_amazon_reviews.csv')\n",
        "\n",
        "    text_column = 'cleaned_text'\n",
        "    label_column = 'sentiment'\n",
        "    df = df.dropna(subset=[text_column])\n",
        "    X = df[text_column].astype(str).values\n",
        "    y = df[label_column].values\n",
        "\n",
        "    print(f\"Data loaded: {len(df)} rows.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find '{CSV_FILE_PATH}'. Please upload it to the Colab files area.\")\n",
        "    raise\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "0mPASQZjkKLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Manual split for oversampling\n",
        "X_train_pure, X_val_pure, y_train_pure, y_val_pure = train_test_split(\n",
        "    X_train_text, y_train,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Pure Training Shape: {X_train_pure.shape}\")\n",
        "print(f\"Pure Validation Shape: {X_val_pure.shape}\")\n",
        "\n",
        "X_train_reshaped = X_train_pure.reshape(-1, 1)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_reshaped, y_train_pure)\n",
        "\n",
        "X_train_resampled = X_train_resampled.flatten()\n",
        "\n",
        "print(f\"Resampled Training Shape: {X_train_resampled.shape}\")\n",
        "print(f\"New Class Distribution: {np.unique(y_train_resampled, return_counts=True)}\")"
      ],
      "metadata": {
        "id": "aeEkZYOH3o2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION & PADDING\n",
        "print(\"Tokenizing text...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "\n",
        "tokenizer.fit_on_texts(X_train_resampled)\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train_resampled)\n",
        "sequences_val = tokenizer.texts_to_sequences(X_val_pure)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test_text)\n",
        "X_train_padded = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_val_padded = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "y_train = y_train_resampled\n",
        "\n",
        "print(f\"Vocab size: {len(tokenizer.word_index)}\")\n",
        "print(f\"Training Data Shape: {X_train_padded.shape}\")\n",
        "print(f\"Validation Data Shape: {X_val_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "L4OZn2K9kNgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARE EMBEDDING MATRIX\n",
        "print(\"Creating embedding matrix...\")\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(MAX_VOCAB_SIZE, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_VOCAB_SIZE: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "id": "ucG80ceKkPr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Bidirectional, LSTM, MaxPooling1D\n",
        "# STRATEGY 3: CNN-LSTM Hybrid + Unfrozen Embeddings\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(MAX_SEQUENCE_LENGTH,)))\n",
        "\n",
        "model.add(Embedding(\n",
        "    num_words,\n",
        "    EMBEDDING_DIM,\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=True\n",
        "))\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# OLD implementation using unidirectional LSTM and Freezing embedding\n",
        "# model = Sequential()\n",
        "# model.add(Input(shape=(MAX_SEQUENCE_LENGTH,)))\n",
        "# model.add(Embedding(\n",
        "#     num_words,\n",
        "#     EMBEDDING_DIM,\n",
        "#     embeddings_initializer=Constant(embedding_matrix),\n",
        "#     trainable=False\n",
        "# ))\n",
        "\n",
        "# model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "WzyA2g4ykSng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_padded, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_padded, y_val_pure),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# OLD training before oversampling Strategy\n",
        "\n",
        "# from sklearn.utils import class_weight\n",
        "# class_weights = class_weight.compute_class_weight(\n",
        "#     class_weight='balanced',\n",
        "#     classes=np.unique(y_train),\n",
        "#     y=y_train\n",
        "# )\n",
        "# class_weights_dict = dict(enumerate(class_weights))\n",
        "# print(f\"Class Weights: {class_weights_dict}\")\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=3,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# history = model.fit(\n",
        "#     X_train_padded, y_train,\n",
        "#     epochs=20,\n",
        "#     batch_size=32,\n",
        "#     validation_split=0.1,\n",
        "#     class_weight=class_weights_dict,\n",
        "#     callbacks=[early_stopping]\n",
        "# )"
      ],
      "metadata": {
        "id": "RDAc66pPkVES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD Evaluation strategy before oversampling where we were forcing the model to be more focused on Neutral class by penalizing it more\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# import numpy as np\n",
        "\n",
        "# loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
        "# print(f\"Standard Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# y_pred_probs = model.predict(X_test_padded)\n",
        "\n",
        "# final_predictions = []\n",
        "\n",
        "# for prob in y_pred_probs:\n",
        "#     # prob[1] is the probability of Neutral\n",
        "#     if prob[1] > 0.35:\n",
        "#         final_predictions.append(1)\n",
        "#     else:\n",
        "#         # If not Neutral, pick the winner between Negative (0) and Positive (2)\n",
        "#         if prob[0] > prob[2]:\n",
        "#             final_predictions.append(0)\n",
        "#         else:\n",
        "#             final_predictions.append(2)\n",
        "\n",
        "# class_names = ['Negative (0)', 'Neutral (1)', 'Positive (2)']\n",
        "# print(classification_report(y_test, final_predictions, target_names=class_names))\n",
        "\n",
        "# loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "# print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# New Evaluation for Oversampling Stragtegy used for 2nd and 3rd strategy\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Generating predictions...\")\n",
        "y_pred_probs = model.predict(X_test_padded)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "class_names = ['Negative (0)', 'Neutral (1)', 'Positive (2)']\n",
        "print(\"Classification Report (Oversampled Model)\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (Oversampled)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNiV7MQ4lcjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating predictions...\")\n",
        "y_pred_probs = model.predict(X_test_padded)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "class_names = ['Negative (0)', 'Neutral (1)', 'Positive (2)']"
      ],
      "metadata": {
        "id": "sqJSMnsCl9Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT TRAINING HISTORY\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'b-', label='Training Acc')\n",
        "    plt.plot(epochs, val_acc, 'r-', label='Validation Acc')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "RoGHSLdgmCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NORMALIZED CONFUSION MATRIX\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix (Normalized by Class)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6aPS4d0wmIO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}